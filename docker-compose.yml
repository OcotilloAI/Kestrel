services:
  kestrel:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - llama-cpp
    environment:
      - LLM_PROVIDER=openai
      - LLM_MODEL=${LLAMA_MODEL_ALIAS:-qwen3-coder}
      - LLM_API_URL=http://llama-cpp:8080
      - GOOSE_WORKDIR=/workspace
      - GOOSE_CONTROLLER_ENABLED=1
    volumes:
      - ./workspace:/workspace
    working_dir: /app

  llama-cpp:
    image: ghcr.io/abetlen/llama-cpp-python:latest
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    command:
      - "python"
      - "-m"
      - "llama_cpp.server"
      - "--model"
      - "${LLAMA_MODEL_PATH:-/models/qwen3-coder-30b-a3b-q4_k_m.gguf}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--n_ctx"
      - "${LLAMA_CTX:-262144}"
      - "--n_gpu_layers"
      - "${LLAMA_N_GPU_LAYERS:--1}"
      - "--alias"
      - "${LLAMA_MODEL_ALIAS:-qwen3-coder}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
